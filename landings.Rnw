\documentclass[12pt,letterpaper]{article}

\usepackage{amsfonts,amsmath,amssymb}
% \usepackage{bibentry}
% \nobibliography*
\usepackage[yyyymmdd]{datetime}
\renewcommand{\dateseparator}{-}
\usepackage[margin=0.8in]{geometry}
\usepackage{gensymb} %to use extra symbols, such as {\degree}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\hypersetup{allcolors=blue,linktocpage,linktoc=all}
\usepackage{multicol}
\usepackage[sort]{natbib}
\renewcommand\bibname{References}
\usepackage{parskip}
\usepackage[section]{placeins} %to use the command \FloatBarrier
\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage} %to start each section with a new page
\usepackage{tocloft} %to have dots in the TOC after sections
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

\newcommand*{\doi}[1]{DOI \href{http://dx.doi.org/#1}{#1}} %to add URLs to DOIs in references


\title{Environmental Drivers of Golden Tilefish Landings\footnote
{These data and code support the results presented in (recommended citation):\vspace*{0.5em}\newline
Nesslage G, Lyubchich V, Nitschke P, Williams E, Grimes C, Wiedenmann J (2020)
Environmental drivers of golden tilefish (\emph{Lopholatilus chamaeleonticeps}) landings and catch per unit effort.
In progress.\vspace*{0.7em}\newline
\hspace*{1.8em}Citation for the current data and code:\vspace*{0.5em}\newline
Lyubchich V, Nesslage G (2020) github.com/vlyubchich/tilefish: Environmental Drivers of Golden Tilefish Landings. Zenodo. DOI 10.5281/zenodo.3743867.  \url{https://doi.org/10.5281/zenodo.3743867}.}
}

\author{Vyacheslav Lyubchich and Genevi\`{e}ve Nesslage\\
{\small Chesapeake Biological Laboratory, University of Maryland Center for Environmental Science,}\\
{\small Solomons, MD, 20688, USA}
}

\date{\today}

\begin{document}
\maketitle
\tableofcontents
\thispagestyle{empty}

<<setup, echo = FALSE>>=
options(digits = 3)
options(width = 110) #width of text output from R
opts_chunk$set(size = 'scriptsize', echo = T, eval = T, fig.width = 7, fig.height = 4,
               out.width = '\\textwidth', fig.show = "hold",
               fig.align = "center", message = FALSE, warning = FALSE)
@



\section{Packages and functions}
<<>>=
rm(list = ls())
#Packages and custom functions used
#All packages are needed, but some are not loaded fully to avoid conflicts
library(Boruta)
# library(dplyr)
# library(forecast)
library(funtimes)
library(GGally)
# library(Hmisc)
library(mgcv)
library(ranger)
library(randomForest)
# library(plotmo)
library(xtable)
@


<<>>=
# Organize summaries for tables
summaryVL = function(X){
  out = sapply(c(1:ncol(X)), function(i) summary(X[,i])[1:6])
  colnames(out) = names(X)
  if(any(is.na(X))){
    Missing = apply(is.na(X), 2, sum)
    out = rbind(out, Missing)
  }
  out
}
@

<<>>=
# Forward-selection of variables for a GAM. See the exact specification in mgcv:: below.
gamforward = function(y, x, data, verbose = TRUE) {
  #y is a character name of the response variable;
  #x is a character vector of x-variables, with most important first (order matters);
  #data is a data frame containing y and x;
  #verbose is a logical values indicating to print out the progress or not.
  K = 5
  alpha = 0.05
  CCthresh = 0.7
  problems = c("non-significance", "concurvity")
  selected = character()
  for(i in 1:length(x)) {
    candidate = x[i]
    problem_check = rep(NA, length(problems))
    f = as.formula(paste0(y, " ~ s(", paste0(c(selected, candidate), collapse = ", k = K) + s("), ", k = K)"))
    z = FALSE
    tryCatch(
      mod  <<-  mgcv::gamm(f
                          , select = TRUE
                          , bs = "cr"
                          , method = "ML"
                          , correlation = corARMA(p = 1, q = 0)
                          , control = lmeControl(msMaxIter = 100)
                          , data = data
      ), error = function(c) {
        z <<- TRUE
        # assign("z", TRUE)
      }
    )
    if (z) {
      if(verbose) {print(paste0("Rejected ", candidate, " due to model non-convergence!!!"))}
    } else {
      #check significance
      fixeff = summary(mod$lme)$tTable[-1, , drop = FALSE]
      pv = fixeff[,ncol(fixeff)]
      problem_check[1] = !all(pv < alpha) #TRUE will indicate a problem
      #check concurvity
      cc = concurvity(mod$gam)[3,-1]
      problem_check[2] = !all(cc < CCthresh) #TRUE will indicate a problem
      if (all(!problem_check)) { #if no problems
        selected = c(selected, candidate)
        if(verbose) {print(paste0("Added ", candidate))}
        # acf(residuals(mod$lme, type = "normalized"), las = 1, main = candidate)
      } else {
        if(verbose) {print(paste0("Rejected ", candidate, " due to ", paste0(problems[problem_check], collapse = " and ")))}
      }
      rm(z)
    }
  }
  print(paste0("Selected variables: ", paste0(selected, collapse = ", ")))
  return(selected)
}
@

Some colors to start with
<<fig.height = 0.5>>=
COL = c(black = "black"
        ,red = rgb(100, 38, 33, maxColorValue = 100)
        ,green = rgb(38, 77, 19, maxColorValue = 100)
        ,blue = rgb(28, 24, 61, maxColorValue = 100)
        ,purple = rgb(76, 32, 72, maxColorValue = 100)
        ,cyan = rgb(21, 75, 87, maxColorValue = 100)
        ,dark_cyan = rgb(0, 47, 59, maxColorValue = 100)
        ,yellow = rgb(99, 90, 13, maxColorValue = 100)
)
par(mar = c(0, 0, 0, 0), mgp = c(0, 0, 0))
barplot(rep(1, length(COL)), col = COL, border = NA, axes = FALSE, space = c(0, 0))
@



\section{Data pre-processing}

\subsection{Load}

<<>>=
D = read.csv("./dataraw/landings.csv", stringsAsFactors = FALSE)
D = D[order(D$Year),]
rownames(D) = D$Year
#convert to the (unordered) factor:
D$Time_block = factor(D$Time_block, levels = unique(D$Time_block))
@

Table~\ref{tab:summNum} gives a summary of the numeric variables (the last row is the number of missing values, i.e., \texttt{NA}'s).
<<>>=
tmp = summaryVL(D[, -3])
@

<<results="asis", echo=FALSE>>=
print(xtable(tmp, label = "tab:summNum",
             caption = "Summary statistics for numeric variables in the dataset"),
      scalebox = 0.66, latex.environments = "center", caption.placement = "top")
@


Table~\ref{tab:summCat} provides a summary of the categorical variables.

<<>>=
tmp = summary(D[, 3, drop = FALSE])
rownames(tmp) = NULL
@


<<results="asis", echo=FALSE>>=
print(xtable(tmp, label = "tab:summCat",
             caption = "Summary statistics for categorical variables in the dataset"),
      scalebox = 0.66, latex.environments = "center", caption.placement = "top")
@


\subsection{Create lagged series, remove FMP period}

\citet{Fisher:etal:2014} suggest that landings are correlated with the Dec--Feb station-based NAO index lagged by up to seven years, so we add those lagged values to the analysis (and lagged values of the other climate indices).

<<>>=
n = names(D)
v = grep("AMO_|NAO_", n) #variables to lag
L = 7 #number of lags to add to variables v
for (i in v) { #i=4
  tmp = sapply(1:L, function(x) dplyr::lag(D[,i], x))
  colnames(tmp) = paste(n[i], "_l", 1:L, sep = "")
  D = cbind(D, tmp)
}
@

Landings data from 2001--2017 (the terminal time block \texttt{FMP}) were removed from the analysis to exclude landings that were quota-limited.

<<>>=
#Save a copy of the full dataset as D0, cut off the FMP period with fishing restrictions
D0 = D
D = D[D$Time_block != "FMP", ]
D$Time_block = droplevels(D$Time_block)
@


\subsection{Tests and visualizations}

The landings time series has few missing values that can be reasonably guessed with linear interpolation (see Figure~\ref{fig:longTS}):
<<>>=
indNA = is.na(D$Landings) #save the location of what we fill-in for later
D$Landings = forecast::na.interp(D$Landings) #interpolate NAs
(TBend = tapply(D$Year, D$Time_block, max)) #end year of each time block
(TBcenter = round(tapply(D$Year, D$Time_block, mean))) #year-center of each time block
@

Try to remove the right skewness using a power transformation (log is too much, use square root):
<<>>=
D$sqrtLandings = sqrt(D$Landings)
@


\begin{figure}
\centering
<<fig.height=7.5>>=
par(mar = c(4, 4, 0.1, 1) + 0.1, mgp = c(3, 0.7, 0), mfrow = c(3, 1))
attach(D)
plot(x = Year, y = Landings, las = 1, type = "o", xlab = "", ylim = c(0, 5000))
points(x = Year[indNA], y = Landings[indNA], bg = COL["red"], pch = 21)
abline(v = TBend + 0.5, lty = 2, col = COL["green"])
text(x = c(1916, TBcenter[-1]), y = 4900, col = COL["green"], labels = names(TBcenter))
plot(x = Year, y = AMO_DJFMA, las = 1, type = "o", xlab = "")
plot(x = Year, y = NAO_DJFMA_PC, las = 1, type = "o")
detach(D)
@
\caption{Plots of the main time series, \Sexpr{min(D$Year)}--\Sexpr{max(D$Year)}. Filled circles represent filled-in (interpolated) values. Dashed vertical lines denote the time blocks.}
\label{fig:longTS}
\end{figure}


Apply a non-parametric test for trends~\citep{Lyubchich:etal:2013:wavk} that is suitable for autocorrelated and conditionally heteroskedastic data.
Specifically, we test the null hypothesis that the unknown trend function $\mu(t)$ in the observed time series is constant (i.e., the hypothesis of no trend) vs.\ the alternative of some other trend (including the cases of a linear, monotonic, or non-linear trend):
\begin{equation}\label{eq:wavkH}
H_0: \mu(t) \equiv 0\quad \text{vs.}\quad H_1: \mu(t) \neq 0.
\end{equation}
<<>>=
set.seed(111)
wavk.test(D$Landings ~ 1,
          factor.length = "adaptive.selection", ar.method = "yw")
wavk.test(D$sqrtLandings ~ 1,
          factor.length = "adaptive.selection", ar.method = "yw")
set.seed(222)
wavk.test(D0$AMO_DJFMA ~ 1,
          factor.length = "adaptive.selection", ar.method = "yw")
wavk.test(D0$AMO_annual ~ 1,
          factor.length = "adaptive.selection", ar.method = "yw")
set.seed(333)
wavk.test(D0$NAO_DJF_PC ~ 1,
          factor.length = "adaptive.selection", ar.method = "yw")
wavk.test(D0$NAO_DJF_st ~ 1,
          factor.length = "adaptive.selection", ar.method = "yw")
wavk.test(D0$NAO_DJFMA_PC ~ 1,
          factor.length = "adaptive.selection", ar.method = "yw")
wavk.test(D0$NAO_DJFMA_st ~ 1,
          factor.length = "adaptive.selection", ar.method = "yw")
@
It is reasonable that the test rejects the $H_0$ for the time series of landings because there are important time blocks of the fisheries that affect the average landings; to account for that we will use the time block variable in the regression models.

See some of the scatterplots in Figure~\ref{fig:pairsLand}.


\begin{figure}
\centering
<<fig.height = 12, fig.width=10>>=
tmp = c(#grep("Time_block", names(D)),
  grep("l4", names(D)),
  grep("Land", names(D)))
ggpairs(D[,tmp])
@
\caption{Matrix of scatterplots, univariate distributions (main diagonal), and pairwise correlations for the period of \Sexpr{min(D$Year)}--\Sexpr{max(D$Year)}. Only some of the lagged variables are plotted for visibility.}
\label{fig:pairsLand}
\end{figure}



\section{Random forest}

References about the methods:
\citet{Breiman:2001,Hastie:etal:2009,Kursa:Rudnicki:2010,Wright:Ziegler:2017}.

Random forest inputs:
<<>>=
NTREE = 500
Nnode = 3
RESPONSE = "sqrtLandings"
RESPONSEprint = "sqrt(Landings)"
#Predictors:
n = names(D)
vnot = grep("Landings|Year", n, value = TRUE) #variables to exclude
predictors = sort(setdiff(n, vnot))
DATA = D
DATAnoNA = na.omit(DATA[,c(RESPONSE, predictors)])
@


Variable selection:
<<>>=
set.seed(444)
B = Boruta::Boruta(as.formula(paste(RESPONSE, ".", sep = " ~ ")),
                   doTrace = 1, maxRuns = 5000,
                   data = DATAnoNA)
print(B)
st = attStats(B) # print(st)
# v = rownames(st)[st$decision != "Rejected"]
v = rownames(st)[st$decision == "Confirmed"]
@

The selected variables in alphabetic order (green in Figure~\ref{fig:Boruta}):
<<>>=
sort(v)
@


\begin{figure}
\centering
<<fig.height = 10, fig.width=9>>=
par(mar = c(4, 9, 0.1, 1) + 0.1)
plot(B, horizontal = TRUE,
     colCode = COL[c("green", "yellow", "red", "blue")],
     las = 1, ylab = "", xlab = "Importance")
@
\caption{Boxplots of predictor importance over runs of the Boruta algorithm. Green boxes correspond to predictors confirmed as important; red boxes correspond to rejected predictors, and blue boxes combine importance information (min, mean, and max) from shadow predictors obtained by permuting the original ones.
Yellow boxes (if any) correspond to predictors without decision (treated as rejected).
}
\label{fig:Boruta}
\end{figure}


\textbf{2nd step of selection} -- use Altmann algorithm. The predictors are the original ones (not conditional on the 1st step).
<<>>=
set.seed(2)
rf_i = ranger(dependent.variable.name = RESPONSE, data = DATAnoNA[,c(RESPONSE, predictors)],
              importance = 'impurity_corrected',
              min.node.size = Nnode, respect.unordered.factors = 'partition',
              num.trees = NTREE)
imp = importance_pvalues(rf_i, method = "altmann",
                         num.permutations = 1000,
                         formula = as.formula(paste(RESPONSE, ".", sep = " ~ ")),
                         data = DATAnoNA[,c(RESPONSE, predictors)])
imp = imp[imp[,2] < 0.05,]
v2 = sort(rownames(imp))
v = sort(intersect(v, v2))
@

The variables selected \textbf{by both} algorithms in alphabetic order:
<<>>=
v
@


The selected variables in order of decreasing importance (i.e., most important first):
<<>>=
w = imp[order(imp[,1], decreasing = TRUE),] #selected by Altmann sorted
w = rownames(w)
(w = w[is.element(w, v)]) #from the Altman-sorted, keep only those that appear in v
@


That is, from the original
\Sexpr{length(predictors)} variables we selected
\Sexpr{length(v)} variables. Get the final random forest with those \Sexpr{length(v)} variables:
<<>>=
set.seed(555)
DATAnoNA = na.omit(DATA[, c(RESPONSE, v)])
rf_allv = ranger(dependent.variable.name = RESPONSE, data = DATAnoNA,
                 #importance = 'impurity_corrected',
                 min.node.size = Nnode,
                 respect.unordered.factors = 'partition',
                 num.trees = NTREE)
print(rf_allv)
@


Non-adjusted R$^2$:
<<>>=
# Not really useful for us
y = DATAnoNA[, RESPONSE]
e = y - predict(rf_allv, data = DATAnoNA)$predictions
1 - sum(e^2) / sum((y - mean(y))^2)
@


Final random forest output from another package (see error plot in Figure~\ref{fig:rftrees} and partial dependence plots in Figure~\ref{fig:partial}):
<<>>=
set.seed(666)
rf_allv2 = randomForest(y = DATAnoNA[,RESPONSE],
                        x = DATAnoNA[, v],
                        nodesize = rf_allv$min.node.size,
                        mtry = rf_allv$mtry,
                        ntree = rf_allv$num.trees)
print(rf_allv2)
@


\begin{figure}
<<>>=
par(mar = c(4, 4, 0.1, 1) + 0.1)
plot(rf_allv2, las = 1, main = "")
@
\caption{Random forest error based on the number of trees.}
\label{fig:rftrees}
\end{figure}


\begin{figure}
<<fig.width=8, fig.height=7>>=
RF = rf_allv2
preds = sort(v)
par(mfrow = c(ceiling(length(preds)/4), 4))
par(bty = "L", mar = c(5, 4, 1, 1) + 0.1, mgp = c(2, 0.7, 0))
for(i in 1:length(preds)) {
  l = ifelse(is.factor(DATAnoNA[,preds[i]]), 2, 1) #rotate labels for categorical predictor
  x = ifelse(is.factor(DATAnoNA[,preds[i]]), "", preds[i]) #x-label
  partialPlot(RF, pred.data = DATAnoNA, x.var = preds[i],
              las = l, xlab = x, ylab = "", main = "", xpd = F
              ,ylim = c(23, 34) #fix y-scale across plots
  )
  mtext(RESPONSEprint, side = 2, line = 2, cex = 0.7)
  mtext(paste("(", letters[i], ")", sep = ""), side = 3, line = 0.1, cex = 0.8, adj = -0.37)
}
@
\caption{Partial dependence plots for the random forest for
\Sexpr{RESPONSEprint} of golden tilefish. The plots are in alphabetic order by the name of the explanatory variable. The inner tickmarks on the horizontal axis denote deciles of the respective explanatory variable.}
\label{fig:partial}
\end{figure}


<<fig.width=11, fig.height=8>>=
n = length(w) #number of single plots to plot
plotmo::plotmo(RF, pmethod = "partdep"
               ,degree1 = paste0("^", w[1:n], "$")
               ,main = c(w[1:n]), caption = "Partial dependence plots"
               ,degree2 = FALSE, type2 = "contour"
               ,las = 1)
@

<<fig.width=11, fig.height=10>>=
plotmo::plotmo(RF, pmethod = "plotmo"
               ,degree1 = paste0("^", w[1:n], "$")
               ,main = c(w[1:n]), caption = "Partial dependence plots"
               ,degree2 = FALSE, type2 = "contour"
               ,las = 1)
@

\FloatBarrier
\subsection{Cross-validation}

Scheme:\label{cvscheme}
\begin{enumerate}
\item Train model using data up to the middle of the \texttt{PostWWII} period.
\item Forecast until the end of that period.
\item Repeat the above steps for the period \texttt{Longline}.
\end{enumerate}

<<>>=
set.seed(777)
CVperiods = c("PostWWII", "Longline")
PRED = as.list(rep(NA, length(CVperiods)))
yrs = as.numeric(rownames(DATAnoNA))
for (cv in seq_along(CVperiods)){ # cv=1
  indtrain = yrs[1]:TBcenter[CVperiods[cv]]
  indtest = (TBcenter[CVperiods[cv]] + 1):TBend[CVperiods[cv]]
  rf_cv = ranger(dependent.variable.name = RESPONSE,
                 data = DATAnoNA[as.character(indtrain),],
                 #importance = 'impurity_corrected',
                 min.node.size = Nnode,
                 respect.unordered.factors = 'partition',
                 num.trees = NTREE)
  obs =  DATAnoNA[as.character(indtest), RESPONSE] #observed in the testing set
  pred = predict(rf_cv, data = DATAnoNA[as.character(indtest),])$predictions #predictions
  PRED[[cv]] = list(obs = obs, pred = pred)
}
@

Count number of test samples per cross-validation run
<<>>=
(tmp = sapply(PRED, function(x) length(x[[1]])))
@
and total
<<>>=
sum(tmp)
@

Prediction mean absolute error (PMAE) and prediction root mean square error (PRMSE).
<<>>=
err = lapply(PRED, function(x) x$obs - x$pred)
err = unlist(err)
# PMAE
mean(abs(err))
# PRMSE
sqrt(mean(err^2))
@


\section{Generalized additive modeling (GAM)}

References about the methods: \citet{Wood:2006book,Zuur:etal:2009}.

\begin{figure}[h]
<<fig.width=11, fig.height=12>>=
ggpairs(DATAnoNA[,c(v, RESPONSE)])
@
\caption{Scatterplot matrix of
\Sexpr{RESPONSEprint} and variables selected in the random forest.}
\label{fig:pairsLand_rf}
\end{figure}


<<>>=
getOption("mgcv.vc.logrange") #25 #see ?mgcv::gamm about convergence
options(mgcv.vc.logrange = 15)
@


\subsection{Backward selection}

Full model:
<<>>=
K = 5
gam_0 = gamfit = mgcv::gam(sqrtLandings ~
                             Time_block
                           + s(AMO_annual_l5, k = K)
                           + s(AMO_annual_l6, k = K)
                           + s(AMO_annual_l7, k = K)
                           + s(AMO_DJFMA_l5, k = K)
                           + s(AMO_DJFMA_l6, k = K)
                           + s(AMO_DJFMA_l7, k = K)
                           + s(NAO_DJF_PC_l4, k = K)
                           + s(NAO_DJF_st_l3, k = K)
                           + s(NAO_DJF_st_l4, k = K)
                           , select = TRUE
                           , bs = "cr"
                           , method = "REML"
                           , data = DATAnoNA)
summary(gamfit)
concurvity(gamfit)[3,-1]
@

Spearman correlations:
<<>>=
Hmisc::rcorr(as.matrix(DATAnoNA[,c(RESPONSE, v[-length(v)])]),
             type = "spearman")[[1]]
@

High concurvity. Most of the concurvity is due to duplicated information (e.g., annual and winter form of a climate index). From each pair of concurve terms, select such term to remove, which is less correlated with the response (use Spearman's correlation coefficient, which allows to account for possibly non-linear monotonic relationship). Based on this approach, we favor the winter form of AMO (keep in the model), and remove the annual AMO version; favor PC-based/station-based NAO and remove station-based/PC-based NAO.

Further variable selection is done by stepwise removal of terms with the largest non-significant $p$-value, until all remaining terms are statistically significant (also see \texttt{?mgcv::gam.selection}).

To additionally incorporate autocorrelation into the model (to get rid of autocorrelation in the model residuals), fit a generalized additive mixed model (GAMM) with autocorrelation structure specified as AR(1) -- autoregression of the order 1 -- and the autoregression coefficient $\phi$ estimated from the data. See the autocorrelation function (ACF) plot of the residuals from the final (reduced) model in Figure~\ref{fig:gam_acf}.

Reduced model:
<<>>=
#cc removed because of concurvity
##1 after that, switched to gamm and AR1 structure and continue removal
set.seed(111111)
K = 5
gam_1 = gamfit = mgcv::gamm(sqrtLandings ~
                         ##1 Time_block
                       #cc + s(AMO_annual_l5, k = K)
                       #cc + s(AMO_annual_l6, k = K)
                       #cc + s(AMO_annual_l7, k = K)
                         ##2 + s(AMO_DJFMA_l5, k = K)
                         ##3 + s(AMO_DJFMA_l6, k = K)
                           + s(AMO_DJFMA_l7, k = K)
                       #cc + s(NAO_DJF_PC_l4, k = K)
                           + s(NAO_DJF_st_l3, k = K)
                           + s(NAO_DJF_st_l4, k = K)
                           , select = TRUE
                           , bs = "cr"
                           , method = "REML"
                           , correlation = corARMA(p = 1, q = 0)
                           , data = DATAnoNA)
@

GAM part:
<<>>=
summary(gamfit$gam)
@

LME part:
<<>>=
summary(gamfit$lme)
@

Normality of residuals:
<<>>=
shapiro.test(residuals(gamfit$lme, type = "normalized"))
shapiro.test(residuals(gamfit$lme, type = "response"))
@


\begin{figure}
\centering
<<fig.height=3>>=
par(mfrow = c(1, 2))
plot.ts(residuals(gamfit$lme, type = "normalized"), las = 1)
abline(h = 0, col = COL["green"], lty = 2)
acf(residuals(gamfit$lme, type = "normalized"), las = 1, main = "Standardized residual ACF")
@
\caption{Time series plot and autocorrelation function of residuals of the reduced GAMM.}
\label{fig:gam_acf}
\end{figure}


Run more checks:
<<>>=
gamfit = gamfit$gam
concurvity(gamfit)
concurvity(gamfit, full = FALSE)$estimate
gam.check(gamfit)
@

\begin{figure}
<<fig.height=3>>=
par(mfrow = c(1, 3))
plot(gamfit, las = 1)
@
\caption{Smoothed terms of the final GAMM for the \Sexpr{RESPONSEprint}.}
\label{fig:gam_smooths}
\end{figure}


\subsection{Forward selection}

Order of selection same as for CPUE (based on importance, not absolute Spearman correlation). Note: forward selection with order based on Spearman (use \texttt{x = tmp} below) gave the same resulting model as the backward selection in the previous section.
<<>>=
fcts = c("Time_block")
w2 = setdiff(w, fcts)
@

<<>>=
tmp = names(sort(abs(Hmisc::rcorr(as.matrix(DATAnoNA[,c(RESPONSE, w2)]),
                                  type = "spearman")[[1]][1,]), decreasing = TRUE))[-1]
gs_all = gamforward(y = RESPONSE,
                    x = w2, #w2 or tmp if want order by |r_spearman|
                    data = DATAnoNA)
K = 5
f = as.formula(paste0(RESPONSE, " ~ s(", paste0(gs_all, collapse = ", k = K) + s("), ", k = K)"))
gam_1forward = gamfit = mgcv::gamm(f
                            , select = TRUE
                            , bs = "cr"
                            , method = "REML"
                            , correlation = corARMA(p = 1, q = 0)
                            , data = DATAnoNA)
# summary(gamfit$gam)
# summary(gamfit$lme)
# concurvity(gamfit$gam)[3,-1]
# acf(residuals(gamfit))
# acf(residuals(gamfit$lme, type = "normalized"), las = 1, main = "Standardized residual ACF")
@

GAM part:
<<>>=
summary(gamfit$gam)
@

LME part:
<<>>=
summary(gamfit$lme)
@

Normality of residuals:
<<>>=
shapiro.test(residuals(gamfit$lme, type = "normalized"))
shapiro.test(residuals(gamfit$lme, type = "response"))
@


\begin{figure}
\centering
<<fig.height=3>>=
par(mfrow = c(1, 2))
plot.ts(residuals(gamfit$lme, type = "normalized"), las = 1)
abline(h = 0, col = COL["green"], lty = 2)
acf(residuals(gamfit$lme, type = "normalized"), las = 1, main = "Standardized residual ACF")
@
\caption{Time series plot and autocorrelation function of residuals of the reduced GAMM (forward selection).}
\label{fig:gam_acf_forward}
\end{figure}


Run more checks:
<<>>=
gamfit = gamfit$gam
concurvity(gamfit)
concurvity(gamfit, full = FALSE)$estimate
gam.check(gamfit)
@

\begin{figure}
<<fig.height=3>>=
par(mfrow = c(1, 3))
plot(gamfit, las = 1)
@
\caption{Smoothed terms of the final GAMM for the \Sexpr{RESPONSEprint}.}
\label{fig:gam_smooths_forward}
\end{figure}



\FloatBarrier
\subsection{Cross-validation}
The scheme is the same as the one applied to random forests (see p.~\pageref{cvscheme}).

<<>>=
set.seed(222222)
CVperiods = c("PostWWII", "Longline")
PRED = as.list(rep(NA, length(CVperiods)))
yrs = as.numeric(rownames(DATAnoNA))
for (cv in seq_along(CVperiods)){ # cv=1
  indtrain = yrs[1]:TBcenter[CVperiods[cv]]
  indtest = (TBcenter[CVperiods[cv]] + 1):TBend[CVperiods[cv]]
  gam_cv = mgcv::gamm(sqrtLandings ~
                        + s(AMO_DJFMA_l7, k = K)
                      + s(NAO_DJF_st_l3, k = K)
                      + s(NAO_DJF_st_l4, k = K)
                      , select = TRUE
                      , bs = "cr" #cr/cs/cc
                      , method = "REML"
                      , correlation = corARMA(p = 1, q = 0)
                      , control = list(maxIter = 1000)
                      , data = DATAnoNA[as.character(indtrain),])
  obs =  DATAnoNA[as.character(indtest), RESPONSE] #observed in the testing set
  # Predictions:
  ## make a prediction, with random effects zero
  pred_gam = predict(gam_cv$gam, newdata = DATAnoNA[as.character(indtest),])
  phi = gam_cv$lme$modelStruct$corStruct
  phi = coef(phi, unconstrained = FALSE) #AR(1) coefficient
  ## extract autocorrelated errors in the training period (we'll need the last observed error)
  fit_gam = fitted(gam_cv$lme)
  e_train = DATAnoNA[as.character(indtrain), RESPONSE] - fit_gam
  #acf(e_train) #ar(e_train, aic = FALSE, order.max = 1) #check that is similar to phi
  e = obs - pred_gam #autocorrelated error in the testing set
  pred_gamm = pred_gam + phi * c(e_train[length(e_train)], e[-length(e)])
  PRED[[cv]] = list(obs = obs, pred_gamm = pred_gamm, pred_gam = pred_gam)
}
@

Prediction mean absolute error (PMAE) and prediction root mean square error (PRMSE).
Note, \texttt{err\_gam} correspond to errors when we do not know the previous month landings, so it is more like $h$-step ahead prediction errors, rather than \texttt{err\_gamm} that use the observed error from the previous month, that is more like 1-step ahead predictions.
<<>>=
err_gam = lapply(PRED, function(x) x$obs - x$pred_gam)
err_gam = unlist(err_gam)
err_gamm = lapply(PRED, function(x) x$obs - x$pred_gamm)
err_gamm = unlist(err_gamm)
# PMAE
mean(abs(err_gam))
mean(abs(err_gamm))
# PRMSE
sqrt(mean(err_gam^2))
sqrt(mean(err_gamm^2))
@



\FloatBarrier
\subsection{Cross-validation of forward-selected model}
The scheme is the same as the one applied to random forests (see p.~\pageref{cvscheme}).

<<>>=
set.seed(222222)
CVperiods = c("PostWWII", "Longline")
PRED = as.list(rep(NA, length(CVperiods)))
yrs = as.numeric(rownames(DATAnoNA))
for (cv in seq_along(CVperiods)){ # cv=1
  indtrain = yrs[1]:TBcenter[CVperiods[cv]]
  indtest = (TBcenter[CVperiods[cv]] + 1):TBend[CVperiods[cv]]
  gam_cv = mgcv::gamm(sqrtLandings ~
                        + s(AMO_DJFMA_l7, k = K)
                      # + s(NAO_DJF_st_l3, k = K)
                      + s(NAO_DJF_st_l4, k = K)
                      , select = TRUE
                      , bs = "cr" #cr/cs/cc
                      , method = "REML"
                      , correlation = corARMA(p = 1, q = 0)
                      , control = list(maxIter = 1000)
                      , data = DATAnoNA[as.character(indtrain),])
  obs =  DATAnoNA[as.character(indtest), RESPONSE] #observed in the testing set
  # Predictions:
  ## make a prediction, with random effects zero
  pred_gam = predict(gam_cv$gam, newdata = DATAnoNA[as.character(indtest),])
  phi = gam_cv$lme$modelStruct$corStruct
  phi = coef(phi, unconstrained = FALSE) #AR(1) coefficient
  ## extract autocorrelated errors in the training period (we'll need the last observed error)
  fit_gam = fitted(gam_cv$lme)
  e_train = DATAnoNA[as.character(indtrain), RESPONSE] - fit_gam
  #acf(e_train) #ar(e_train, aic = FALSE, order.max = 1) #check that is similar to phi
  e = obs - pred_gam #autocorrelated error in the testing set
  pred_gamm = pred_gam + phi * c(e_train[length(e_train)], e[-length(e)])
  PRED[[cv]] = list(obs = obs, pred_gamm = pred_gamm, pred_gam = pred_gam)
}
@

Prediction mean absolute error (PMAE) and prediction root mean square error (PRMSE).
Note, \texttt{err\_gam} correspond to errors when we do not know the previous month landings, so it is more like $h$-step ahead prediction errors, rather than \texttt{err\_gamm} that use the observed error from the previous month, that is more like 1-step ahead predictions.
<<>>=
err_gam = lapply(PRED, function(x) x$obs - x$pred_gam)
err_gam = unlist(err_gam)
err_gamm = lapply(PRED, function(x) x$obs - x$pred_gamm)
err_gamm = unlist(err_gamm)
# PMAE
mean(abs(err_gam))
mean(abs(err_gamm))
# PRMSE
sqrt(mean(err_gam^2))
sqrt(mean(err_gamm^2))
@


<<>>=
save.image(paste0("./dataderived/tmp_", Sys.Date(), ".RData"))
@


\addcontentsline{toc}{section}{\bibname}
\bibliographystyle{latex/spbasic}
\bibliography{latex/refTilefish}
\end{document}



